Index: segment/train_seg.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import torch\r\nfrom torch.utils.data import DataLoader\r\nimport torch.optim as optim\r\nimport torch.optim.lr_scheduler as lr_scheduler\r\nfrom tensorboardX import SummaryWriter\r\nimport time\r\nimport sys\r\n\r\nsys.path.append('..')\r\nfrom segment.utils.dataset import *\r\nfrom segment.model.model import *\r\nfrom segment.utils import utils, parser\r\nfrom segment.utils.visualize import *\r\n\r\ntorch.autograd.set_detect_anomaly(True)\r\n\r\n\r\ndef train(args, writer):\r\n    train_dataset = PartDataset(data_path=args.data_path, cates=args.dataset,\r\n                                raw_data=None,\r\n                                split='train', scale_mode=args.scale_mode, transform=None)\r\n    train_loader = DataLoader(train_dataset, batch_size=args.train_batch_size, num_workers=1)\r\n    model = SegGen(args)\r\n    pre_encoder = torch.load(args.start_ckpts_encoder)\r\n    model.encoder.load_state_dict(pre_encoder)\r\n    for param in model.encoder.parameters():\r\n        param.requires_grad = False\r\n\r\n    model = model.cuda()\r\n    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\r\n                           lr=args.lr)\r\n    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.max_epoch)\r\n\r\n    print(repr(model))\r\n    print(args)\r\n    # Criterion\r\n    criterion = model.get_loss\r\n\r\n    for epoch in range(args.max_epoch):\r\n        # train\r\n        losses = train_one_epoch(args, model, train_loader, optimizer, criterion, epoch)\r\n        scheduler.step()\r\n\r\n        writer.add_scalar('Epoch/loss_emd', losses.avg(0), epoch)\r\n        writer.add_scalar('Epoch/loss_cd', losses.avg(1), epoch)\r\n        writer.add_scalar('Epoch/loss_loc', losses.avg(2), epoch)\r\n        writer.add_scalar('Epoch/loss_bal', losses.avg(3), epoch)\r\n        writer.add_scalar('Epoch/loss_rank', losses.avg(4), epoch)\r\n\r\n        if (epoch + 1) % args.ckpt_save_freq == 0:\r\n            filename = os.path.join(args.log_file, f'model_{epoch}.pth')\r\n            print(f'Saving checkpoint to: {filename}')\r\n            torch.save(model.state_dict(), filename)\r\n\r\n\r\ndef train_one_epoch(args, model, train_loader, optimizer, criterion, epoch):\r\n    losses = utils.AverageMeter(\r\n        ['loss_emd', 'loss_cd', 'loss_loc', 'loss_bal', 'loss_rank'])\r\n    n_batches = len(train_loader)\r\n    model.train()\r\n\r\n    for i, data in enumerate(train_loader):\r\n        args.batch_size = data['pointcloud'].shape[0]\r\n        points = data['pointcloud'].cuda()\r\n        # gt_label = data['labels'].cuda()\r\n        part_recon, recon_all, p_feat, part_feat, sp_atten, pre_label, means, logvars = model(args, points)\r\n\r\n        part_points = utils.sample_aprt_point(args, pre_label, points)\r\n\r\n        loss_emd, loss_cd, loss_loc, loss_bal, loss_rank \\\r\n            = criterion(points, part_points, part_recon, part_feat,\r\n                        p_feat, sp_atten, means, logvars)\r\n        loss = loss_cd + loss_rank + loss_loc + 0.01 * loss_bal\r\n        loss /= args.batch_size\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        optimizer.step()\r\n\r\n        # summary\r\n        losses.update([loss_emd.item(), loss_cd.item(),\r\n                       loss_loc.item(), loss_bal.item(),\r\n                       loss_rank.item()])\r\n\r\n        if ((i + 1) % (n_batches // 4) == 0) & (epoch % 40 == 0):\r\n            save_path = data['cate'][0] + '_' + str(np.array(data['id'][0]))\r\n            vis_part = torch.stack([points[0] for points in part_recon], dim=0)\r\n            vis_recon = torch.concat([points[0] for points in part_recon], dim=0)\r\n            write_ply_with_color(os.path.join(args.log_file, save_path + \"_recon.ply\"),\r\n                                 vis_part.cpu().detach().numpy())\r\n            vis_cate(points[0].cpu().detach().numpy(), pre_label[0].cpu().detach().numpy(), args,\r\n                     save_path=os.path.join(args.log_file, save_path + \"_cate.ply\"))\r\n            # vis_cate(points[0].cpu().detach().numpy(), gt_label[0].cpu().detach().numpy(), args,\r\n            #          save_path=os.path.join(args.log_file, save_path + \"_gt.ply\"))\r\n\r\n        torch.cuda.empty_cache()\r\n\r\n    print('[Training] EPOCH: %d Losses = %s' % (\r\n        epoch, [(name, '%.4f' % value) for name, value in zip(losses.items, losses.avg())]))\r\n\r\n    return losses\r\n\r\n\r\nif __name__ == '__main__':\r\n    args = parser.get_args()\r\n    timestamp = time.strftime('%Y-%m-%d-%H-%M-%S', time.localtime())\r\n    args.log_file = os.path.join(args.log_dir, args.dataset, f'{timestamp}')\r\n    writer = SummaryWriter(args.log_file)\r\n    train(args, writer)\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/segment/train_seg.py b/segment/train_seg.py
--- a/segment/train_seg.py	(revision 9e6d293f85c4e0c9d232af09c09663a901ba6782)
+++ b/segment/train_seg.py	(date 1717503586590)
@@ -1,4 +1,3 @@
-import torch
 from torch.utils.data import DataLoader
 import torch.optim as optim
 import torch.optim.lr_scheduler as lr_scheduler
Index: segment/model/model.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import sys\r\nimport torch.nn.init as init\r\nfrom chamfer_distance import ChamferDistance as chamfer_dist\r\n\r\nsys.path.append('..')\r\nfrom script.PyTorchEMD import emd\r\nfrom segment.model.pointnet import *\r\nfrom segment.model.partae import *\r\n\r\n\r\nclass SegGen(nn.Module):\r\n    def __init__(self, args):\r\n        super().__init__()\r\n        self.part_num = args.part_num\r\n        self.temp = 10\r\n        self.encoder = PointNetEncoder(args)\r\n        self.atten_encoder = PointNetEncoder(args)\r\n\r\n        self.attention_layer = nn.Sequential(\r\n            nn.Conv1d(256, self.part_num, 1)\r\n        )\r\n        self.decoder = PartDecoder(args)\r\n        # self.sp_atten = nn.Parameter(torch.rand(args.data_point, args.part_num))\r\n        # init.xavier_uniform_(self.sp_atten)\r\n\r\n    def forward(self, args, points):\r\n        p_feat = self.encoder(points)  # B N C\r\n        sp_atten = self.attention_layer(p_feat.transpose(2, 1))  # B 50(sp num) N\r\n        sp_atten = F.softmax(sp_atten, dim=1)\r\n\r\n        part_feat = torch.bmm(F.normalize(sp_atten, p=1, dim=2), p_feat)\r\n        pre_label = torch.argmax(sp_atten.transpose(1, 2), axis=-1)\r\n\r\n        part_recon, recon_all, means, logvars = self.decoder(part_feat)\r\n        return part_recon, recon_all, p_feat, part_feat, sp_atten, pre_label, means, logvars\r\n\r\n    def get_loss(self, points, part_points, part_recon, part_feat,\r\n                 p_feat, sp_atten, means, logvars):\r\n        B, N, C = p_feat.shape\r\n        _, M, _ = sp_atten.shape\r\n\r\n        # loc loss\r\n        centriods = torch.bmm(F.normalize(sp_atten, p=1, dim=2), points)  # B M 3\r\n        cent_un = centriods.unsqueeze(2)  # B M 1 3\r\n        points_un = points.unsqueeze(1)  # B 1 N 3\r\n        coord_dist = cent_un - points_un  # B M N 3\r\n        coord_dist = torch.norm(coord_dist, dim=-1)  # B M N\r\n        coord_dist = coord_dist * sp_atten  # B M N\r\n        loss_loc = torch.sum(coord_dist)  # paper\r\n\r\n        # sp balance loss\r\n        sp_atten_per_sp = torch.sum(sp_atten, dim=-1)  # B M\r\n        sp_atten_sum = torch.sum(sp_atten_per_sp, dim=-1, keepdim=True) / M  # B 1\r\n        loss_bal = torch.sum((sp_atten_per_sp - sp_atten_sum) ** 2) / M\r\n\r\n        # loss recon\r\n        loss_emd = 0\r\n        loss_cd = 0\r\n        chd = chamfer_dist()\r\n\r\n        for i in range(len(part_recon)):\r\n            loss_emd += emd.earth_mover_distance(part_recon[i].transpose(2, 1).to('cuda'),\r\n                                                 part_points[i].transpose(2, 1).to('cuda')).sum()\r\n            dist1, dist2, idx1, idx2 = chd(part_points[i].to('cuda'), part_recon[i].to('cuda'))\r\n            loss_cd += (torch.mean(dist1)) + (torch.mean(dist2))\r\n\r\n        # loss_kl = 0\r\n        # for mean, logvar in zip(means, logvars):\r\n        #     loss_kl += -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\r\n\r\n        # ce = nn.CrossEntropyLoss()\r\n        # gt_label = (gt_label - 1).clone().long()\r\n        # loss_ce = ce(sp_atten, gt_label)\r\n\r\n        part_feat = part_feat.transpose(0, 1)\r\n        lowRankLoss = torch.zeros([self.part_num], dtype=torch.float).cuda()\r\n        for i in range(self.part_num):\r\n            _, s, _ = torch.svd(part_feat[i, :, :], some=False)\r\n            lowRankLoss[i] = s[1] / s[0]\r\n\r\n        highRankLoss = torch.zeros([int((self.part_num - 1) * self.part_num / 2)], dtype=torch.float).cuda()\r\n        idx = 0\r\n        for i in range(self.part_num):\r\n            for j in range(self.part_num):\r\n                if j <= i:\r\n                    continue\r\n                _, s, _ = torch.svd(torch.cat([part_feat[i, :, :], part_feat[j, :, :]], 1), some=False)\r\n                highRankLoss[idx] = s[1] / s[0]\r\n                idx = idx + 1\r\n        loss_rank = 1 + torch.max(lowRankLoss) - torch.min(highRankLoss)\r\n\r\n        return loss_emd, loss_cd, loss_loc, loss_bal, loss_rank\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/segment/model/model.py b/segment/model/model.py
--- a/segment/model/model.py	(revision 9e6d293f85c4e0c9d232af09c09663a901ba6782)
+++ b/segment/model/model.py	(date 1717503587494)
@@ -3,7 +3,7 @@
 from chamfer_distance import ChamferDistance as chamfer_dist
 
 sys.path.append('..')
-from script.PyTorchEMD import emd
+from extention.PyTorchEMD import emd
 from segment.model.pointnet import *
 from segment.model.partae import *
 
Index: script/src/cuda_helper.h
===================================================================
diff --git a/script/src/cuda_helper.h b/script/src/cuda_helper.h
deleted file mode 100644
--- a/script/src/cuda_helper.h	(revision 9e6d293f85c4e0c9d232af09c09663a901ba6782)
+++ /dev/null	(revision 9e6d293f85c4e0c9d232af09c09663a901ba6782)
@@ -1,18 +0,0 @@
-#ifndef CUDA_HELPER_H_
-#define CUDA_HELPER_H_
-
-#define CUDA_CHECK(err) \
-	if (cudaSuccess != err) \
-	{ \
-		fprintf(stderr, "CUDA kernel failed: %s (%s:%d)\n", \
-			cudaGetErrorString(err),  __FILE__, __LINE__); \
-		std::exit(-1); \
-	}
-
-#define CHECK_CUDA(x) AT_CHECK(x.type().is_cuda(), \
-	#x " must be a CUDA tensor")
-#define CHECK_CONTIGUOUS(x) AT_CHECK(x.is_contiguous(), \
-	#x " must be contiguous")
-#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)
-
-#endif
\ No newline at end of file
Index: script/src/metrics.cpp
===================================================================
diff --git a/script/src/metrics.cpp b/script/src/metrics.cpp
deleted file mode 100644
--- a/script/src/metrics.cpp	(revision 9e6d293f85c4e0c9d232af09c09663a901ba6782)
+++ /dev/null	(revision 9e6d293f85c4e0c9d232af09c09663a901ba6782)
@@ -1,517 +0,0 @@
-// torch library headers
-#include <torch/extension.h>
-#include <THC/THC.h>
-
-// C++ standard header
-#include <algorithm>
-#include <vector>
-#include <math.h>
-#include <omp.h>
-#include <cstdio>
-#include <iostream>
-#include <memory>
-
-// CUDA and/or cuBLAS header
-#include "cuda_helper.h"
-#include <cuda_runtime.h>
-#include <cublas_v2.h>
-#include <cusolver_common.h>
-#include <cusolverDn.h>
-
-// Mark: CUDA EMD primal form (through sinkhorn iteration) from Optas github
-void approxmatchLauncher(int b,
-    int n,
-    int m,
-    const float * xyz1,
-    const float * xyz2,
-    float * match,
-    float * temp);
-
-void matchcostLauncher(int b,
-    int n,
-    int m,
-    const float * xyz1,
-    const float * xyz2,
-    const float * match,
-    float * out);
-
-void matchcostgradLauncher(int b,
-    int n,
-    int m,
-    const float * xyz1,
-    const float * xyz2,
-    const float * match,
-    float * grad1,
-    float * grad2);
-
-// Mark: CUDA Chamfer distance
-int ChamferDistanceKernelLauncher(
-    const int b, const int n,
-    const float* xyz,
-    const int m,
-    const float* xyz2,
-    float* result,
-    int* result_i,
-    float* result2,
-    int* result2_i);
-
-int ChamferDistanceGradKernelLauncher(
-    const int b, const int n,
-    const float* xyz1,
-    const int m,
-    const float* xyz2,
-    const float* grad_dist1,
-    const int* idx1,
-    const float* grad_dist2,
-    const int* idx2,
-    float* grad_xyz1,
-    float* grad_xyz2);
-
-std::vector<at::Tensor> emd_distance_forward_cuda(const at::Tensor xyz1, 
-    const at::Tensor xyz2) 
-{
-	// Allocate necessary data structures
-	at::Tensor match = at::zeros({xyz1.size(0), xyz1.size(1), xyz2.size(1)}, 
-		xyz1.options());
-	at::Tensor cost = at::zeros({xyz1.size(0)}, xyz1.options());
-	at::Tensor temp = at::zeros({xyz1.size(0), 2 * (xyz1.size(1) + xyz2.size(1))}, 
-		xyz1.options());
-
-	// Find the approximate matching 
-	approxmatchLauncher(xyz1.size(0), 
-    xyz1.size(1), 
-    xyz2.size(1),
-    xyz1.data<float>(),
-    xyz2.data<float>(),
-	match.data<float>(),
-    temp.data<float>());
-
-	// Compute the matching cost
-	matchcostLauncher(xyz1.size(0), 
-    xyz1.size(1), 
-    xyz2.size(1),
-    xyz1.data<float>(),
-    xyz2.data<float>(),
-	match.data<float>(),
-	cost.data<float>());
-
-    return {cost, match};
-}
-
-// CUDA 
-
-std::vector<at::Tensor> emd_distance_backward_cuda(const at::Tensor xyz1,
-    const at::Tensor xyz2,
-    const at::Tensor match)
-{
-	// Allocate necessary data structures
-	at::Tensor gradxyz1 = at::zeros_like(xyz1);
-	at::Tensor gradxyz2 = at::zeros_like(xyz2);
-
-    matchcostgradLauncher(xyz1.size(0), 
-    xyz1.size(1), 
-    xyz2.size(1), 
-    xyz1.data<float>(),
-    xyz2.data<float>(),
-    match.data<float>(), 
-    gradxyz1.data<float>(), 
-    gradxyz2.data<float>());
-    
-    // return gradients
-    return {gradxyz1, gradxyz2};
-}
-
-void chamfer_distance_forward_cuda(
-    const at::Tensor xyz1, 
-    const at::Tensor xyz2, 
-    const at::Tensor dist1, 
-    const at::Tensor dist2, 
-    const at::Tensor idx1, 
-    const at::Tensor idx2) 
-{
-    ChamferDistanceKernelLauncher(xyz1.size(0),
-    xyz1.size(1),
-    xyz1.data<float>(),
-    xyz2.size(1),
-    xyz2.data<float>(),
-    dist1.data<float>(), 
-    idx1.data<int>(),
-    dist2.data<float>(), 
-    idx2.data<int>());
-}
-
-void chamfer_distance_backward_cuda(
-    const at::Tensor xyz1,
-    const at::Tensor xyz2, 
-    at::Tensor gradxyz1, 
-    at::Tensor gradxyz2, 
-    at::Tensor graddist1, 
-    at::Tensor graddist2, 
-    at::Tensor idx1, 
-    at::Tensor idx2)
-{
-    ChamferDistanceGradKernelLauncher(xyz1.size(0),
-    xyz1.size(1),
-    xyz1.data<float>(),
-    xyz2.size(1),
-    xyz2.data<float>(),
-    graddist1.data<float>(), 
-    idx1.data<int>(),
-    graddist2.data<float>(), 
-    idx2.data<int>(),
-    gradxyz1.data<float>(), gradxyz2.data<float>());
-}
-
-
-
-//'''
-//
-//CPU function wrappers!!!
-//'''
-
-
-// Mark: Wasserstein (EMD) cpu function
-
-void approxmatch_cpu(int b,int n,int m,const float * xyz1,const float * xyz2,float * match){
-	for (int i=0;i<b;i++){
-		int factorl=std::max(n,m)/n;
-		int factorr=std::max(n,m)/m;
-		std::vector<double> saturatedl(n,double(factorl)),saturatedr(m,double(factorr));
-		std::vector<double> weight(n*m);
-		for (int j=0;j<n*m;j++)
-			match[j]=0;
-		for (int j=8;j>=-2;j--){
-			//printf("i=%d j=%d\n",i,j);
-			double level=-powf(4.0,j);
-			if (j==-2)
-				level=0;
-			for (int k=0;k<n;k++){
-				double x1=xyz1[k*3+0];
-				double y1=xyz1[k*3+1];
-				double z1=xyz1[k*3+2];
-				for (int l=0;l<m;l++){
-					double x2=xyz2[l*3+0];
-					double y2=xyz2[l*3+1];
-					double z2=xyz2[l*3+2];
-					weight[k*m+l]=expf(level*((x1-x2)*(x1-x2)+(y1-y2)*(y1-y2)+(z1-z2)*(z1-z2)))*saturatedr[l];
-				}
-			}
-			std::vector<double> ss(m,1e-9);
-			for (int k=0;k<n;k++){
-				double s=1e-9;
-				for (int l=0;l<m;l++){
-					s+=weight[k*m+l];
-				}
-				for (int l=0;l<m;l++){
-					weight[k*m+l]=weight[k*m+l]/s*saturatedl[k];
-				}
-				for (int l=0;l<m;l++)
-					ss[l]+=weight[k*m+l];
-			}
-			for (int l=0;l<m;l++){
-				double s=ss[l];
-				double r=std::min(saturatedr[l]/s,1.0);
-				ss[l]=r;
-			}
-			std::vector<double> ss2(m,0);
-			for (int k=0;k<n;k++){
-				double s=0;
-				for (int l=0;l<m;l++){
-					weight[k*m+l]*=ss[l];
-					s+=weight[k*m+l];
-					ss2[l]+=weight[k*m+l];
-				}
-				saturatedl[k]=std::max(saturatedl[k]-s,0.0);
-			}
-			for (int k=0;k<n*m;k++)
-				match[k]+=weight[k];
-			for (int l=0;l<m;l++){
-				saturatedr[l]=std::max(saturatedr[l]-ss2[l],0.0);
-			}
-		}
-		xyz1+=n*3;
-		xyz2+=m*3;
-		match+=n*m;
-	}
-}
-
-void matchcost_cpu(int b,int n,int m,const float * xyz1,const float * xyz2,const float * match,float * cost){
-	for (int i=0;i<b;i++){
-		double s=0;
-		for (int j=0;j<n;j++)
-			for (int k=0;k<m;k++){
-				float x1=xyz1[j*3+0];
-				float y1=xyz1[j*3+1];
-				float z1=xyz1[j*3+2];
-				float x2=xyz2[k*3+0];
-				float y2=xyz2[k*3+1];
-				float z2=xyz2[k*3+2];
-				float d=sqrtf((x2-x1)*(x2-x1)+(y2-y1)*(y2-y1)+(z2-z1)*(z2-z1))*match[j*m+k];
-				s+=d;
-			}
-		cost[0]=s;
-		xyz1+=n*3;
-		xyz2+=m*3;
-		match+=n*m;
-		cost+=1;
-	}
-}
-
-void matchcostgrad_cpu(int b,int n,int m,const float * xyz1,const float * xyz2,const float * match,float * grad1,float * grad2){
-	for (int i=0;i<b;i++){
-		for (int j=0;j<n;j++)
-			grad1[j*3+0]=0;
-		for (int j=0;j<m;j++){
-			float sx=0,sy=0,sz=0;
-			for (int k=0;k<n;k++){
-				float x2=xyz2[j*3+0];
-				float y2=xyz2[j*3+1];
-				float z2=xyz2[j*3+2];
-				float x1=xyz1[k*3+0];
-				float y1=xyz1[k*3+1];
-				float z1=xyz1[k*3+2];
-				float d=std::max(sqrtf((x2-x1)*(x2-x1)+(y2-y1)*(y2-y1)+(z2-z1)*(z2-z1)),1e-20f);
-				float dx=match[k*m+j]*((x2-x1)/d);
-				float dy=match[k*m+j]*((y2-y1)/d);
-				float dz=match[k*m+j]*((z2-z1)/d);
-				grad1[k*3+0]-=dx;
-				grad1[k*3+1]-=dy;
-				grad1[k*3+2]-=dz;
-				sx+=dx;
-				sy+=dy;
-				sz+=dz;
-			}
-			grad2[j*3+0]=sx;
-			grad2[j*3+1]=sy;
-			grad2[j*3+2]=sz;
-		}
-		xyz1+=n*3;
-		xyz2+=m*3;
-		match+=n*m;
-		grad1+=n*3;
-		grad2+=m*3;
-	}
-}
-
-// Mark: Chamfer distance cpu function
-
-void nnsearch(
-    const int b, const int n, const int m,
-    const float* xyz1,
-    const float* xyz2,
-    float* dist,
-    int* idx)
-{
-    for (int i = 0; i < b; i++) {
-        for (int j = 0; j < n; j++) {
-            const float x1 = xyz1[(i*n+j)*3+0];
-            const float y1 = xyz1[(i*n+j)*3+1];
-            const float z1 = xyz1[(i*n+j)*3+2];
-            double best = 0;
-            int besti = 0;
-            for (int k = 0; k < m; k++) {
-                const float x2 = xyz2[(i*m+k)*3+0] - x1;
-                const float y2 = xyz2[(i*m+k)*3+1] - y1;
-                const float z2 = xyz2[(i*m+k)*3+2] - z1;
-                const double d=x2*x2+y2*y2+z2*z2;
-                if (k==0 || d < best){
-                    best = d;
-                    besti = k;
-                }
-            }
-            dist[i*n+j] = best;
-            idx[i*n+j] = besti;
-        }
-    }
-}
-
-
-
-//* batch Euclidean grid cpu
-void varifold_kernel_cpu(int b,
-	int n,
-	int m,
-	const float * xyz1,
-	const float * xyz2, 
-	const float * nor1, 
-	const float * nor2,
-	float * match){
-	for (int i=0;i<b;i++){
-		std::vector<double> weight(n*m);
-		for (int k=0;k<n;k++){
-			double x1 =xyz1[k*3+0];
-			double y1 =xyz1[k*3+1];
-			double z1 =xyz1[k*3+2];
-			double nx1=nor1[k*3+0];
-			double ny1=nor1[k*3+1];
-			double nz1=nor1[k*3+2];
-			for (int l=0;l<m;l++){
-				double x2 =xyz2[l*3+0];
-				double y2 =xyz2[l*3+1];
-				double z2 =xyz2[l*3+2];
-				double nx2=nor2[l*3+0];
-				double ny2=nor2[l*3+1];
-				double nz2=nor2[l*3+2];
-				match[k*m+l]=std::max(expf(-1.0*((x1-x2)*(x1-x2)+(y1-y2)*(y1-y2)+(z1-z2)*(z1-z2)))*powf(nx1*nx2+ny1*ny2+nz1*nz2,2),1e-10f);
-			}
-		}
-		xyz1+=n*3;
-		xyz2+=m*3;
-		nor1+=n*3;
-		nor2+=m*3;
-		match+=n*m;
-	}
-}
-
-std::vector<at::Tensor>  emd_distance_forward(
-    const at::Tensor xyz1, 
-    const at::Tensor xyz2){
-
-	// Allocate necessary data structures
-	at::Tensor match = at::zeros({xyz1.size(0), xyz1.size(1), xyz2.size(1)}, 
-		xyz1.options());
-	at::Tensor cost = at::zeros({xyz1.size(0)}, xyz1.options());
-	// Find the approximate matching 
-
-	approxmatch_cpu(xyz1.size(0), 
-    xyz1.size(1), 
-    xyz2.size(1),
-    xyz1.data<float>(),
-    xyz2.data<float>(),
-	match.data<float>());
-
-	// Compute the matching cost
-	matchcost_cpu(xyz1.size(0), 
-    xyz1.size(1), 
-    xyz2.size(1),
-    xyz1.data<float>(),
-    xyz2.data<float>(),
-	match.data<float>(),
-	cost.data<float>());
-
-    // return output
-	return {cost, match};
-}
-
-std::vector<at::Tensor>  emd_distance_backward(
-    const at::Tensor xyz1, 
-    const at::Tensor xyz2,
-    const at::Tensor match){
-
-	// Allocate necessary data structures
-	at::Tensor gradxyz1 = at::zeros_like(xyz1);
-	at::Tensor gradxyz2 = at::zeros_like(xyz2);
-
-    matchcostgrad_cpu(xyz1.size(0), 
-    xyz1.size(1), 
-    xyz2.size(1), 
-    xyz1.data<float>(),
-    xyz2.data<float>(),
-    match.data<float>(), 
-    gradxyz1.data<float>(), 
-    gradxyz2.data<float>());    
-
-    // return gradients
-    return {gradxyz1, gradxyz2};
-
-}
-
-void chamfer_distance_forward(
-    const at::Tensor xyz1, 
-    const at::Tensor xyz2, 
-    const at::Tensor dist1, 
-    const at::Tensor dist2, 
-    const at::Tensor idx1, 
-    const at::Tensor idx2) 
-{
-    const int batchsize = xyz1.size(0);
-    const int n = xyz1.size(1);
-    const int m = xyz2.size(1);
-
-    const float* xyz1_data = xyz1.data<float>();
-    const float* xyz2_data = xyz2.data<float>();
-    float* dist1_data = dist1.data<float>();
-    float* dist2_data = dist2.data<float>();
-    int* idx1_data = idx1.data<int>();
-    int* idx2_data = idx2.data<int>();
-
-    nnsearch(batchsize, n, m, xyz1_data, xyz2_data, dist1_data, idx1_data);
-    nnsearch(batchsize, m, n, xyz2_data, xyz1_data, dist2_data, idx2_data);
-}
-
-
-void chamfer_distance_backward(
-    const at::Tensor xyz1, 
-    const at::Tensor xyz2, 
-    at::Tensor gradxyz1, 
-    at::Tensor gradxyz2, 
-    at::Tensor graddist1, 
-    at::Tensor graddist2, 
-    at::Tensor idx1, 
-    at::Tensor idx2) 
-{
-    const int b = xyz1.size(0);
-    const int n = xyz1.size(1);
-    const int m = xyz2.size(1);
-
-    const float* xyz1_data = xyz1.data<float>();
-    const float* xyz2_data = xyz2.data<float>();
-    float* gradxyz1_data = gradxyz1.data<float>();
-    float* gradxyz2_data = gradxyz2.data<float>();
-    float* graddist1_data = graddist1.data<float>();
-    float* graddist2_data = graddist2.data<float>();
-    const int* idx1_data = idx1.data<int>();
-    const int* idx2_data = idx2.data<int>();
-
-    for (int i = 0; i < b*n*3; i++)
-        gradxyz1_data[i] = 0;
-    for (int i = 0; i < b*m*3; i++)
-        gradxyz2_data[i] = 0;
-    for (int i = 0;i < b; i++) {
-        for (int j = 0; j < n; j++) {
-            const float x1 = xyz1_data[(i*n+j)*3+0];
-            const float y1 = xyz1_data[(i*n+j)*3+1];
-            const float z1 = xyz1_data[(i*n+j)*3+2];
-            const int j2 = idx1_data[i*n+j];
-
-            const float x2 = xyz2_data[(i*m+j2)*3+0];
-            const float y2 = xyz2_data[(i*m+j2)*3+1];
-            const float z2 = xyz2_data[(i*m+j2)*3+2];
-            const float g = graddist1_data[i*n+j]*2;
-
-            gradxyz1_data[(i*n+j)*3+0] += g*(x1-x2);
-            gradxyz1_data[(i*n+j)*3+1] += g*(y1-y2);
-            gradxyz1_data[(i*n+j)*3+2] += g*(z1-z2);
-            gradxyz2_data[(i*m+j2)*3+0] -= (g*(x1-x2));
-            gradxyz2_data[(i*m+j2)*3+1] -= (g*(y1-y2));
-            gradxyz2_data[(i*m+j2)*3+2] -= (g*(z1-z2));
-        }
-        for (int j = 0; j < m; j++) {
-            const float x1 = xyz2_data[(i*m+j)*3+0];
-            const float y1 = xyz2_data[(i*m+j)*3+1];
-            const float z1 = xyz2_data[(i*m+j)*3+2];
-            const int j2 = idx2_data[i*m+j];
-            const float x2 = xyz1_data[(i*n+j2)*3+0];
-            const float y2 = xyz1_data[(i*n+j2)*3+1];
-            const float z2 = xyz1_data[(i*n+j2)*3+2];
-            const float g = graddist2_data[i*m+j]*2;
-            gradxyz2_data[(i*m+j)*3+0] += g*(x1-x2);
-            gradxyz2_data[(i*m+j)*3+1] += g*(y1-y2);
-            gradxyz2_data[(i*m+j)*3+2] += g*(z1-z2);
-            gradxyz1_data[(i*n+j2)*3+0] -= (g*(x1-x2));
-            gradxyz1_data[(i*n+j2)*3+1] -= (g*(y1-y2));
-            gradxyz1_data[(i*n+j2)*3+2] -= (g*(z1-z2));
-        }
-    }
-}
-
-PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
-    m.def("cd_forward", &chamfer_distance_forward, "Chamfer Distance forward");
-    m.def("cd_forward_cuda", &chamfer_distance_forward_cuda, "ChamferDistance forward (CUDA)");
-    m.def("cd_backward", &chamfer_distance_backward, "Chamfer Distance backward");
-    m.def("cd_backward_cuda", &chamfer_distance_backward_cuda, "ChamferDistance backward (CUDA)");
-    m.def("emd_distance_forward", &emd_distance_forward, "Wasserstein (Earth Mover's) Distance forward");
-    m.def("emd_distance_forward_cuda", &emd_distance_forward_cuda, "Wasserstein (Earth Mover's) Distance forward (CUDA)");
-    m.def("emd_distance_backward", &emd_distance_backward, "Wasserstein (Earth Mover's) Distance backward");
-    m.def("emd_distance_backward_cuda", &emd_distance_backward_cuda, "Wasserstein (Earth Mover's) Distance backward (CUDA)");
-}
Index: script/metrics/pytorch_structural_losses/src/utils.hpp
===================================================================
diff --git a/script/metrics/pytorch_structural_losses/src/utils.hpp b/script/metrics/pytorch_structural_losses/src/utils.hpp
deleted file mode 100644
--- a/script/metrics/pytorch_structural_losses/src/utils.hpp	(revision 9e6d293f85c4e0c9d232af09c09663a901ba6782)
+++ /dev/null	(revision 9e6d293f85c4e0c9d232af09c09663a901ba6782)
@@ -1,26 +0,0 @@
-#include <iostream>
-#include <sstream>
-#include <string>
-
-class Formatter {
-public:
-  Formatter() {}
-  ~Formatter() {}
-
-  template <typename Type> Formatter &operator<<(const Type &value) {
-    stream_ << value;
-    return *this;
-  }
-
-  std::string str() const { return stream_.str(); }
-  operator std::string() const { return stream_.str(); }
-
-  enum ConvertToString { to_str };
-
-  std::string operator>>(ConvertToString) { return stream_.str(); }
-
-private:
-  std::stringstream stream_;
-  Formatter(const Formatter &);
-  Formatter &operator=(Formatter &);
-};
Index: generate/model/model.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from chamfer_distance import ChamferDistance as chamfer_dist\r\nimport torch.nn.init as init\r\nimport sys\r\nsys.path.append('..')\r\nfrom script.PyTorchEMD import emd\r\nfrom generate.model.pointnet import *\r\nfrom generate.model.partae import *\r\n\r\n\r\nclass SegGen(nn.Module):\r\n    def __init__(self, args=None):\r\n        super().__init__()\r\n        self.part_num = args.part_num\r\n        self.encoder = PointNetEncoder(args)\r\n        self.decoder = PointNetDecoder(args)\r\n        self.sp_atten = nn.Parameter(torch.rand(args.data_point, args.part_num))\r\n        init.xavier_uniform_(self.sp_atten)\r\n\r\n    def forward(self, args, points):\r\n        p_feat = self.encoder(points)\r\n        sp_atten = F.softmax(self.sp_atten, dim=1)\r\n        part_feat = torch.matmul(p_feat.transpose(1, 2), sp_atten).transpose(1, 2)\r\n        recon_points = self.decoder(part_feat)\r\n        return recon_points, p_feat, sp_atten\r\n\r\n    def get_loss(self, points, recon, pre_label, gt_label):\r\n        # recon loss\r\n        loss_emd = emd.earth_mover_distance(recon.transpose(2, 1), points.transpose(2, 1)).sum()\r\n\r\n        chd = chamfer_dist()\r\n        dist1, dist2, idx1, idx2 = chd(points, recon)\r\n        loss_cd = (torch.mean(dist1)) + (torch.mean(dist2))\r\n\r\n        ce = nn.CrossEntropyLoss()\r\n        gt_label = (gt_label-1).clone().long()\r\n        rep_pre_label = pre_label.repeat(gt_label.shape[0], 1, 1).float()\r\n        loss_ce = ce(rep_pre_label.reshape(-1, self.part_num), gt_label.reshape(-1))\r\n\r\n        return loss_emd, loss_ce, loss_cd\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/generate/model/model.py b/generate/model/model.py
--- a/generate/model/model.py	(revision 9e6d293f85c4e0c9d232af09c09663a901ba6782)
+++ b/generate/model/model.py	(date 1717503586585)
@@ -2,7 +2,7 @@
 import torch.nn.init as init
 import sys
 sys.path.append('..')
-from script.PyTorchEMD import emd
+from extention.PyTorchEMD import emd
 from generate.model.pointnet import *
 from generate.model.partae import *
 
Index: .gitignore
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>/data/\r\n/generate/logs\r\n/segment/logs\r\n/script/metrics\r\n/script/pyTorchChamferDistance\r\n/.idea\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.gitignore b/.gitignore
--- a/.gitignore	(revision 9e6d293f85c4e0c9d232af09c09663a901ba6782)
+++ b/.gitignore	(date 1717503669465)
@@ -1,6 +1,6 @@
 /data/
 /generate/logs
 /segment/logs
-/script/metrics
-/script/pyTorchChamferDistance
+/extention/PyTorchEMD
+/extention/pyTorchChamferDistance
 /.idea
diff --git a/script/PyTorchEMD/emd_ext.egg-info/dependency_links.txt b/extention/PyTorchEMD/emd_ext.egg-info/dependency_links.txt
rename from script/PyTorchEMD/emd_ext.egg-info/dependency_links.txt
rename to extention/PyTorchEMD/emd_ext.egg-info/dependency_links.txt
diff --git a/script/PyTorchEMD/emd_ext.egg-info/top_level.txt b/extention/PyTorchEMD/emd_ext.egg-info/top_level.txt
rename from script/PyTorchEMD/emd_ext.egg-info/top_level.txt
rename to extention/PyTorchEMD/emd_ext.egg-info/top_level.txt
diff --git a/script/PyTorchEMD/emd_ext.egg-info/SOURCES.txt b/extention/PyTorchEMD/emd_ext.egg-info/SOURCES.txt
rename from script/PyTorchEMD/emd_ext.egg-info/SOURCES.txt
rename to extention/PyTorchEMD/emd_ext.egg-info/SOURCES.txt
diff --git a/script/PyTorchEMD/setup.py b/extention/PyTorchEMD/setup.py
rename from script/PyTorchEMD/setup.py
rename to extention/PyTorchEMD/setup.py
diff --git a/script/PyTorchEMD/emd.py b/extention/PyTorchEMD/emd.py
rename from script/PyTorchEMD/emd.py
rename to extention/PyTorchEMD/emd.py
diff --git a/script/PyTorchEMD/README.md b/extention/PyTorchEMD/README.md
rename from script/PyTorchEMD/README.md
rename to extention/PyTorchEMD/README.md
diff --git a/script/PyTorchEMD/__init__.py b/extention/PyTorchEMD/__init__.py
rename from script/PyTorchEMD/__init__.py
rename to extention/PyTorchEMD/__init__.py
diff --git a/script/PyTorchEMD/test_emd_loss.py b/extention/PyTorchEMD/test_emd_loss.py
rename from script/PyTorchEMD/test_emd_loss.py
rename to extention/PyTorchEMD/test_emd_loss.py
diff --git a/script/PyTorchEMD/cuda/emd.cpp b/extention/PyTorchEMD/cuda/emd.cpp
rename from script/PyTorchEMD/cuda/emd.cpp
rename to extention/PyTorchEMD/cuda/emd.cpp
diff --git a/script/utils.py b/extention/utils.py
rename from script/utils.py
rename to extention/utils.py
